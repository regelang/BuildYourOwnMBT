/// ================================================================================
/// æ­å–œä½ å·²ç»å®Œæˆäº†Identifierå’ŒKeywordçš„è¯æ³•åˆ†æï¼Œç°åœ¨æˆ‘ä»¬æ¥å®ç°ä¸€ä¸‹æ³¨é‡Šçš„è·³è¿‡ã€‚
/// 
/// é€šå¸¸æ¥è¯´ï¼Œä¸€ä¸ªåˆæ ¼çš„ç¼–ç¨‹è¯­è¨€éƒ½æ”¯æŒæ³¨é‡Šï¼Œç”¨æ¥ç»™ä»£ç æ·»åŠ è¯´æ˜å’Œæ–‡æ¡£ã€‚
///
/// æˆ‘ä»¬åªéœ€è¦å®ç°ä¸€ç§æ³¨é‡Šå½¢å¼ï¼šå•è¡Œæ³¨é‡Šï¼Œä½¿ç”¨ `//` å¼€å¤´ï¼Œç›´åˆ°è¡Œå°¾ç»“æŸã€‚
///
/// å½“è¯æ³•åˆ†æå™¨é‡åˆ° `//` æ—¶ï¼Œå®ƒåº”è¯¥å¿½ç•¥è¯¥è¡Œå‰©ä½™çš„æ‰€æœ‰å†…å®¹ï¼Œç›´åˆ°é‡åˆ°æ¢è¡Œç¬¦æˆ–æ–‡ä»¶ç»“å°¾ã€‚
/// è¿™æ ·ï¼Œæ³¨é‡Šå°±ä¸ä¼šè¢«è½¬æ¢ä¸º Tokenï¼Œä»è€Œä¸ä¼šå½±å“åç»­çš„è¯­æ³•åˆ†æå’Œç¼–è¯‘è¿‡ç¨‹ã€‚
///
/// ç°åœ¨ç»§ç»­ä¿®æ”¹`lexer/lexer.mbt`æ–‡ä»¶ï¼Œå®Œæˆ`tokenize`å‡½æ•°å§ã€‚
///
/// åœ¨ä½ å®ç°å®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨moon testå‘½ä»¤æ¥è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼š
///
/// æˆ–è€…ç”¨ä¸‹é¢çš„å‘½ä»¤æ¥å•ç‹¬è¿è¡Œå½“å‰æµ‹è¯•ï¼š
///
/// ```
/// moon test -p lexer -f tokenize_comments_test.mbt
/// ```
///
/// ç¥ä½ æˆåŠŸï¼ğŸš€
/// ================================================================================

///|
test "Skip Comments" {
  let code =
    #|fnx fn letp mut  // First line
    #|if else struct // Second line
    #|while return    // Third line
  let tokens = tokenize(code)
  assert_true(tokens.length() is 10)
  assert_true(tokens[0].kind is Lower("fnx"))
  assert_true(tokens[1].kind is Keyword(Fn))
  assert_true(tokens[2].kind is Lower("letp"))
  assert_true(tokens[3].kind is Keyword(Mut))
  assert_true(tokens[4].kind is Keyword(If))
  assert_true(tokens[5].kind is Keyword(Else))
  assert_true(tokens[6].kind is Keyword(Struct))
  assert_true(tokens[7].kind is Keyword(While))
  assert_true(tokens[8].kind is Keyword(Return))
  assert_true(tokens[9].kind is EOF)
}
