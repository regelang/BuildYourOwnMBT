/// ================================================================================
/// æ­å–œä½ ï¼Œè‡³æ­¤æˆ‘ä»¬å·²ç»å®Œæˆäº†å¤§éƒ¨åˆ†Tokençš„è¯æ³•åˆ†æäº†ï¼Œç°åœ¨æˆ‘ä»¬æ¥å®Œæˆå­—ç¬¦ä¸²çš„è¯æ³•åˆ†æã€‚
///
/// åœ¨MiniMoonBitä¸­ï¼Œå­—ç¬¦ä¸²æ˜¯ç”±""åŒ…è£¹çš„ä»»æ„æ–‡æœ¬ï¼Œä¾‹å¦‚"hello"ï¼Œ"123", "..."ã€‚
///
/// å½“æˆ‘ä»¬é‡åˆ°ç¬¬ä¸€ä¸ª"æ—¶ï¼Œå¼€å¯ä¸€ä¸ªå¾ªç¯ï¼Œä¸€ç›´è¯»å–åé¢çš„æ‰€æœ‰å­—ç¬¦ï¼Œç›´åˆ°å†æ¬¡é‡åˆ°"ä¸ºæ­¢ã€‚
///
/// åœ¨çœŸæ­£çš„å·¥ä¸šçº§ç¼–è¯‘ä¸­ï¼Œéœ€è¦æ³¨æ„å¤„ç†è½¬ä¹‰å­—ç¬¦çš„æƒ…å†µï¼Œä¾‹å¦‚"hello\"world"ä¸­çš„\"ä¸åº”è¯¥è¢«å½“åšå­—ç¬¦ä¸²çš„ç»“å°¾ã€‚
/// æ­¤å¤–è¿˜éœ€è¦å¤„ç†æ¢è¡Œç¬¦ç­‰æƒ…å†µã€‚
///
/// ä¸è¿‡ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬è¿™é‡Œçš„MiniMoonbitä¸å¤„ç†è½¬ä¹‰å­—ç¬¦ï¼Œå½“ä½ è¯»å–åˆ°"æ—¶ï¼Œå°±å¯ä»¥è®¤ä¸ºå­—ç¬¦ä¸²ç»“æŸäº†ã€‚
///
/// ç°åœ¨ç»§ç»­ä¿®æ”¹`lexer/lexer.mbt`æ–‡ä»¶ï¼Œå®Œæˆ`tokenize`å‡½æ•°å§ã€‚
///
/// åœ¨ä½ å®ç°å®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨moon testå‘½ä»¤æ¥è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼š
///
/// æˆ–è€…ç”¨ä¸‹é¢çš„å‘½ä»¤æ¥å•ç‹¬è¿è¡Œå½“å‰æµ‹è¯•ï¼š
///
/// ```
/// moon test -p lexer -f tokenize_string_test.mbt
/// ```
///
/// ç¥ä½ æˆåŠŸï¼ğŸš€
/// ================================================================================

///|
test "Tokenize String" {
  let code =
    #|"Hello"
    #|"This is a test string"
    #|"Hello world!"
    #|"MiniMoonbit2025"
  let tokens = tokenize(code)
  assert_true(tokens.length() is 5)
  assert_true(tokens[0].kind is String("Hello"))
  assert_true(tokens[1].kind is String("This is a test string"))
  assert_true(tokens[2].kind is String("Hello world!"))
  assert_true(tokens[3].kind is String("MiniMoonbit2025"))
  assert_true(tokens[4].kind is EOF)
}
